{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARATÓN BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFÍO 2: TORTUGA CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En proyectos de ciencia de datos destinados a construir modelos de *aprendizaje automático*, o aprendizaje estadístico, es muy inusual que los datos iniciales ya estén en el formato ideal para la construcción de modelos. Se requieren varios pasos intermedios de preprocesamiento de datos, como la codificación de variables categóricas, normalización de variables numéricas, tratamiento de datos faltantes, etc. La biblioteca **scikit-learn**, una de las bibliotecas de código abierto más populares para *aprendizaje automático* en el mundo, ya tiene varias funciones integradas para realizar las transformaciones de datos más utilizadas. Sin embargo, en un flujo común de un modelo de aprendizaje automático, es necesario aplicar estas transformaciones al menos dos veces: la primera vez para \"entrenar\" el modelo, y luego nuevamente cuando se envían nuevos datos como entrada para ser clasificados por este modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "# Primero, realizamos la instalación de scikit-learn\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación importaremos varias bibliotecas que se utilizarán:\n",
    "\n",
    "# Biblioteca para trabajar con JSON\n",
    "import json\n",
    "\n",
    "# Biblioteca para realizar solicitudes HTTP\n",
    "import requests\n",
    "\n",
    "# Biblioteca para exploración y análisis de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Biblioteca con métodos numéricos y representaciones matriciales\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Paquetes scikit-learn para preprocesamiento de datos\n",
    "# \"SimpleImputer\" es una transformación para completar los valores faltantes en conjuntos de datos\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Paquetes de scikit-learn para entrenamiento de modelos y construcción de pipelines\n",
    "# Método para separar el conjunto de datos en muestras de testes y entrenamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Método para crear modelos basados en árboles de decisión\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Clase para crear una pipeline de machine-learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Paquetes scikit-learn para evaluación de modelos\n",
    "# Métodos para la validación cruzada del modelo creado\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar  un .csv a tu proyecto en IBM Cloud Pak for Data al Kernel de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, importaremos el conjunto de datos proporcionado para el desafío, que ya está incluido en este proyecto.\n",
    "\n",
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/maratonadev-la/desafio-2-2020/master/Assets/Data/dataset-tortuga-desafio-2.csv\n",
    "df_training_dataset = pd.read_csv(r'dataset-tortuga-desafio-2.csv')\n",
    "df_training_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 16 columnas presentes en el set de datos proporcionado, 15 de las cuales son variables features (datos de entrada) y una de ellas es una variable target (que queremos que nuestro modelo va a predecir).\n",
    "\n",
    "Las variables features son:\n",
    "\n",
    "    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n",
    "    NAME                                - Nombre del estudiante\n",
    "    USER_ID                             - Número de identificación del estudiante\n",
    "    HOURS_DATASCIENCE                   - Número de horas de estudio en Data Science\n",
    "    HOURS_BACKEND                       - Número de horas de estudio en Web (Back-End)\n",
    "    HOURS_FRONTEND                      - Número de horas de estudio en Web (Front-End)\n",
    "    NUM_COURSES_BEGINNER_DATASCIENCE    - Número de cursos de nivel principiante en Data Science completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_BACKEND        - Número de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_FRONTEND       - Número de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_DATASCIENCE    - Número de cursos de nivel avanzado en Data Science completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_BACKEND        - Número de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_FRONTEND       - Número de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n",
    "    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n",
    "    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n",
    "    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n",
    "    \n",
    "La variable target es:\n",
    "\n",
    "    PROFILE                             - Perfil de carrera del estudiante (puede ser uno de 6)\n",
    "    \n",
    "        - beginner_front_end\n",
    "        - advanced_front_end\n",
    "        - beginner_back_end\n",
    "        - advanced_back_end\n",
    "        - beginner_data_science\n",
    "        - advanced_data_science\n",
    "        \n",
    "Con un modelo capaz de clasificar a un alumno en una de estas categorías, podemos recomendar contenidos a los alumnos de forma personalizada según las necesidades de cada alumno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando los datos proporcionados\n",
    "\n",
    "Podemos continuar la exploración de los datos proporcionados con la función ``info()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización (visualizations)\n",
    "\n",
    "Para ver el conjunto de datos suministrado, podemos usar las bibliotecas ``matplotlib`` y ``seaborn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_training_dataset['HOURS_DATASCIENCE'].dropna(), ax=axes[0])\n",
    "sns.distplot(df_training_dataset['HOURS_BACKEND'].dropna(), ax=axes[1])\n",
    "sns.distplot(df_training_dataset['HOURS_FRONTEND'].dropna(), ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_DATASCIENCE'].dropna(), ax=axes[0][0])\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_BACKEND'].dropna(), ax=axes[0][1])\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_FRONTEND'].dropna(), ax=axes[0][2])\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_DATASCIENCE'].dropna(), ax=axes[1][0])\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_BACKEND'].dropna(), ax=axes[1][1])\n",
    "sns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_FRONTEND'].dropna(), ax=axes[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n",
    "\n",
    "sns.distplot(df_training_dataset['AVG_SCORE_DATASCIENCE'].dropna(), ax=axes[0])\n",
    "sns.distplot(df_training_dataset['AVG_SCORE_BACKEND'].dropna(), ax=axes[1])\n",
    "sns.distplot(df_training_dataset['AVG_SCORE_FRONTEND'].dropna(), ax=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(28, 4))\n",
    "\n",
    "sns.countplot(ax=axes[0], x='PROFILE', data=df_training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_corre = df_training_dataset.corr()\n",
    "display(mat_corre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como se puede ver no existe correlacion entre las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral_correlacion = 0.6\n",
    "df_cols = mat_corre.stack().reset_index()\n",
    "df_cols.columns = ['VariableA','VariableB','correlation']\n",
    "df_corr = df_cols[(df_cols['correlation']>umbral_correlacion)&(df_cols['correlation']!=1)]\n",
    "display(df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el preprocesamiento de los datos, se presentarán en este notebook dos transformaciones básicas, demostrando la construcción de un Pipeline con un modelo funcional. Este Pipeline funcional provisto debe ser mejorado por el participante para que el modelo final alcance la mayor precisión posible, garantizando una mayor puntuación en el desafío. Esta mejora solo se puede realizar en el preprocesamiento de los datos, en la elección de un algoritmo para el entrenamiento de diferentes modelos, o incluso en la alteración del **framework** utilizado (sin embargo, solo se entregará un ejemplo de integración de Watson Machine Learning con *scikit-learn*).\n",
    "\n",
    "La primera transformación (paso en nuestro Pipeline) será la exclusión de la columna \"NOMBRE\" de nuestro conjunto de datos, que además de no ser una variable numérica, tampoco es una variable relacionada con el desempeño de los estudiantes en las disciplinas. Hay funciones listas para usar en *scikit-learn* para realizar esta transformación, sin embargo, nuestro ejemplo demostrará cómo crear una transformación personalizada desde cero en scikit-learn. Si lo desea, el participante puede usar este ejemplo para crear otras transformaciones y agregarlas al Pipeline final :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 1: excluir columnas del conjunto de datos\n",
    "\n",
    "Para la creación de una transformación de datos personalizada en scikit-learn, es necesario crear una clase con los métodos ``transform`` y ``fit``. En el método de 'transform', se ejecutará la lógica de nuestra transformación.\n",
    "\n",
    "La siguiente celda muestra el código completo de una transformación ``DropColumns`` para eliminar columnas de un pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# All sklearn Transforms must have the `transform` and `fit` methods\n",
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Primero copiamos el dataframe de datos de entrada 'X'\n",
    "        data = X.copy()\n",
    "        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n",
    "        return data.drop(labels=self.columns, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar esa transformación en un pandas DataFrame pandas, basta instanciar un objeto *DropColumns* y llamar el método transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la transformación ``DropColumns`` al conjunto de datos base\n",
    "df_training_dataset_2 = DropColumns(columns=[\"NAME\", \"Unnamed: 0\",\"USER_ID\"]).fit_transform(X=df_training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las columnas del conjunto de datos transformado\n",
    "print(\"Columnas del conjunto de datos después de la transformación ``DropColumns``: \\n\")\n",
    "print(df_training_dataset_2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que la columna \"NOMBRE\" se ha eliminado y nuestro conjunto de datos ahora solo tiene 14 columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación 2: tratamiento de datos faltantes\n",
    "\n",
    "Para manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformación lista para usar de la biblioteca scikit-learn, llamada **SimpleImputer**.\n",
    "\n",
    "Esta transformación permite varias estrategias para el tratamiento de datos faltantes. La documentación oficial se puede encontrar en: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "\n",
    "En este ejemplo, simplemente haremos cero todos los valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos faltantes del conjunto de datos antes de la primera transformación (df_data_2)\n",
    "print(\"Valores nulos antes de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset_2.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformación)\n",
    "df_training_dataset_3 = DataFrameImputer().fit_transform(df_training_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los datos faltantes del conjunto de datos después de la segunda transformación (SimpleImputer) (df_data_3)\n",
    "print(\"Valores nulos en el conjunto de datos después de la transformación SimpleImputer: \\n\\n{}\\n\".format(df_training_dataset_3.isnull().sum(axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScaleImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Primero copiamos el dataframe de datos de entrada 'X'\n",
    "        data = X.copy()\n",
    "        num_features = data.columns.drop(self.columns)\n",
    "        transformer = ColumnTransformer(transformers=[('scaler', StandardScaler(),num_features)])\n",
    "        X_transform = transformer.fit_transform(data)\n",
    "        X_imputed_df = pd.DataFrame(data = X_transform, columns = num_features.values)\n",
    "        X_imputed_df[self.columns] = data[self.columns]\n",
    "        return X_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset_4 = DataScaleImputer(columns=['PROFILE']).fit_transform(df_training_dataset_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que ya no tenemos valores perdidos en nuestro conjunto de datos :)\n",
    "\n",
    "Vale la pena señalar que cambiar los valores perdidos por 0 no siempre es la mejor estrategia. Se anima al participante a estudiar e implementar diferentes estrategias para tratar los valores perdidos para mejorar su modelo y mejorar su puntuación final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando un modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez finalizado el preprocesamiento, ya tenemos el conjunto de datos en el formato necesario para entrenar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_dataset_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo proporcionado, usaremos todas las columnas, excepto la columna **Profile** como *feautres* (variables de entrada).\n",
    "\n",
    "La variable **Profile** será la variable objetivo del modelo, como se describe en la declaración de desafío."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de features del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de las columnas que seran features (Notese que la columna NOMBRE no esta presente)\n",
    "features = [\n",
    "    \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n",
    "    \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n",
    "    \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n",
    "    \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n",
    "]\n",
    "\n",
    "# Definición de variable objetivo\n",
    "target = ['PROFILE']\n",
    "X = df_training_dataset_4[features]\n",
    "y = df_training_dataset_4[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables (X e y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separar el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separaremos el conjunto de datos provisto en dos grupos: uno para entrenar nuestro modelo y otro para probar el resultado a través de una prueba ciega. La separación del conjunto de datos se puede hacer fácilmente con el método *train_test_split ()* de scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en conjunto de entrenamiento y conjunto de pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un modelo ensamble - Entrenando varios modelos de clasificación previo Tunnig ver sección al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla_aleatoria = 1234\n",
    "class_SVC = SVC(C=1000,kernel='rbf',random_state=semilla_aleatoria,gamma='auto')\n",
    "class_RFO = RandomForestClassifier(n_estimators=800, min_samples_split=2,min_samples_leaf= 1,max_features= 'auto',max_depth=30, bootstrap=True, random_state=semilla_aleatoria)\n",
    "class_RNN = MLPClassifier(solver= 'sgd', max_iter= 480, learning_rate= 'adaptive', hidden_layer_sizes= (144, 144, 12), alpha= 0.05, activation='tanh', random_state=semilla_aleatoria)\n",
    "dtc_model = VotingClassifier(estimators=[('SVC', class_SVC), ('RFOR', class_RFO),('RNN',class_RNN)], voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material teórico sobre árboles de decisión en la documentación oficial de scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "Una guía para principiantes del mundo del aprendizaje automático: https://developer.ibm.com/es/patterns/use-icp4d-to-build-the-machine-learning-model-for-return-propensity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecucion del entrenamiento del árbol de descisión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de modelos (llamado método *fit ()* con conjuntos de entrenamiento)\n",
    "dtc_model.fit(\n",
    "    X_train,\n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de predicciones y evaluación del modelo creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realización de una prueba a ciegas en el modelo creado\n",
    "y_pred = dtc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Precisión lograda por el árbol de decisiones\n",
    "print(\"model score: %.3f\" % dtc_model.score(X_test, y_test))\n",
    "print(\"Exactitud: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning de los Hiperparametros de los Modelos\n",
    "## Correr únicamente si se desea hacer una mejora en los hiper parametros\n",
    "##### Cambiar esta celda a CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cambiar esta celda a CODE\n",
    "parameter_space = {'SVC__C':[0.1, 1, 10, 100, 1000],\n",
    "    'SVC__kernel' : ['linear', 'rbf', 'poly'],\n",
    "    'RFOR__n_estimators':  [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'RFOR__max_features': ['auto', 'sqrt'],\n",
    "    'RFOR__max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'RFOR__min_samples_split': [2, 5, 10],\n",
    "    'RFOR__min_samples_leaf':  [1, 2, 4],\n",
    "    'RFOR__bootstrap': [True, False],        \n",
    "    'RNN__hidden_layer_sizes': [(12,12,6),(144,144,12),(12,144,6),(24,432,12),(24,144,24)],\n",
    "    'RNN__activation': ['tanh', 'relu'],\n",
    "    'RNN__solver': ['sgd', 'adam'],\n",
    "    'RNN__alpha': [0.0001, 0.05,0.02],\n",
    "    'RNN__max_iter': [int(x) for x in np.linspace(start = 100, stop = 2000, num = 6)],              \n",
    "    'RNN__learning_rate': ['constant','adaptive'],   \n",
    "}\n",
    "\n",
    "clf_CV = RandomizedSearchCV(dtc_model, parameter_space, n_jobs=-1, cv=5)\n",
    "clf_CV.fit(X_train, y_train) \n",
    "print('Los parametros optimizados son:\\n', clf_CV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puntuación de los datos necesarios para entregar la solución\n",
    "\n",
    "Como entrega de su solución, esperamos que los resultados se clasifiquen en el siguiente dataset llamado \"to_be_scored_tortuga_fix.csv\":\n",
    "\n",
    "### Descarga la \"hoja de evaluación\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/vanderlei-test/dataset-tortuga/master/to_be_scored_tortuga_fix.csv\n",
    "df_to_be_scored = pd.read_csv(r'to_be_scored_tortuga_fix.csv')\n",
    "df_to_be_scored.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Atención!\n",
    "\n",
    "El Dataframe ``to_be_scored_tortuga`` es su \"hoja de evaluación\". Tenga en cuenta que la columna \"PROFILE\" hace falta en este dataset de evaluación, por lo que no se puede usar para entrenar modelos de aprendizaje supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_scored.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# ¡Atención!\n",
    "\n",
    "# Para aplicar su modelo y clasificar la hoja de evaluación, primero debe aplicar las mismas transformaciones de columnas que aplicó al conjunto de datos de entrenamiento.\n",
    "\n",
    "# No elimine ni agregue líneas a la hoja de respuestas.\n",
    "\n",
    "# No cambie el orden de las líneas en la hoja de respuestas.\n",
    "\n",
    "# Al final, se deben clasificar las 500 entradas, con los valores proporcionados en la columna \"target\"\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda de abajo, repetimos rápidamente los mismos pasos de preprocesamiento usados ​​en el ejemplo dado con el árbol de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_scored_2 = DropColumns(columns=[\"NAME\", \"Unnamed: 0\",\"USER_ID\"]).fit_transform(df_to_be_scored) \n",
    "df_to_be_scored_3 = DataFrameImputer().fit_transform(df_to_be_scored_2) \n",
    "df_to_be_scored_4 = DataScaleImputer(columns=[]).fit_transform(df_to_be_scored_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realización de una prueba a ciegas en el modelo creado\n",
    "y_pred = dtc_model.predict(df_to_be_scored_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregando las respuestas en la columna \"target\"\n",
    "df_to_be_scored_4['target'] = y_pred\n",
    "df_to_be_scored_4.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar la hoja de respuestas como un archivo .csv para enviar\n",
    "**SI YA TIENE UN DATA ASSET CON EL NOMBRE `results.csv` EN ESTE PROYECTO O EN OTRO PROYECTO DE WATSON STUDIO DEBE BORRAR EL ARCHIVO ANTES DE CORRER LA SIGUIENTE CELDA O TENDRA EL ERROR: _RuntimeError: File 'results.csv' already exists in storage._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(file_name=\"results.csv\", data=df_to_be_scored_4.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atención\n",
    "\n",
    "# La ejecución de la celda anterior creará un nuevo \"data asset\" en su proyecto de Watson Studio. Deberá descargar este archivo junto con este cuaderno y crear un archivo zip con **results.csv** y **notebook.ipynb** para enviarlo. (los archivos deben tener este nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## ¡Felicitaciones!\n",
    "\n",
    "Si ya está satisfecho con su solución, vaya a la página siguiente y envíe los archivos necesarios para su envío.\n",
    "\n",
    "# https://tortuga.maratona.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
