{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARATONA BEHIND THE CODE 2020\n",
    "\n",
    "## DESAFIO 2: PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la parte 1 de este desafío, se realizó el procesamiento previo y el entrenamiento de un modelo a partir de un conjunto de datos base proporcionados. En este segundo paso, se integrará todas las transformaciones y eventos de entrenamiento creados previamente en un Pipeline completo para *deploy* en **Watson Machine Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero realizaremos la instalación do scikit-learn y la importación de las mismas bibliotecas utilizadas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==0.20.0 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario volver a insertar el conjunto de datos base como un pandas dataframe, siguiendo las instrucciones\n",
    "\n",
    "![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n",
    "\n",
    "Después de seleccionar la opción **\"Insert to code\"**, la celda de abajo se llenará con el código necesario para importar y leer los datos en el archivo .csv como un Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data_1 = pd.read_csv(r'dataset-tortuga-desafio-2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del Pipeline completo para el encapsulamiento en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando transformaciones personalizadas para cargar en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paso anterior, se mostró cómo crear una transformación personalizada, declarando una clase Python con los métodos ``fit`` y ``transform``.\n",
    "\n",
    "    - Código de transformación personalizada DropColumns():\n",
    "    \n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    # All sklearn Transforms must have the `transform` and `fit` methods\n",
    "    class DropColumns(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, columns):\n",
    "            self.columns = columns\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            # Primero copiamos el dataframe de entrada 'X' de entrada\n",
    "            data = X.copy()\n",
    "            # Devolvemos un nuevo marco de datos sin las columnas no deseadas\n",
    "            return data.drop(labels=self.columns, axis='columns')\n",
    "\n",
    "Para integrar estos tipos de transformaciones personalizadas con Pipelines en Watson Machine Learning, primero debe empaquetar su código personalizado como una biblioteca de Python. Esto se puede hacer fácilmente usando la herramienta *setuptools*.\n",
    "\n",
    "En el siguiente repositorio de git: https://github.com/vnderlev/sklearn_transforms tenemos todos los archivos necesarios para crear un paquete de Python, llamado **my_custom_sklearn_transforms**.\n",
    "Este paquete tiene la siguiente estructura de archivos:\n",
    "\n",
    "    /my_custom_sklearn_transforms.egg-info\n",
    "        dependency_links.txt\n",
    "        not-zip-safe\n",
    "        PKG-INFO\n",
    "        SOURCES.txt\n",
    "        top_level.txt\n",
    "    /my_custom_sklearn_transforms\n",
    "        __init__.py\n",
    "        sklearn_transformers.py\n",
    "    PKG-INFO\n",
    "    README.md\n",
    "    setup.cfg\n",
    "    setup.py\n",
    "    \n",
    "El archivo principal, que contendrá el código para nuestras transformaciones personalizadas, es el archivo **/my_custom_sklearn_transforms/sklearn_transformers.py**. Si accedes a él en el repositorio, notarás que contiene exactamente el mismo código declarado en el primer paso (la clase DropColumns).\n",
    "\n",
    "Si has declarado sus propias transformaciones (además de la DropColumn proporcionada), debes agregar todas las clases de esas transformaciones creadas en este mismo archivo. Para hacer esto, debes hacer fork de este repositorio (esto se puede hacer en la propia interfaz web de Github, haciendo clic en el botón como se muestra en la imagen a continuación) y agregue sus clases personalizadas al archivo **sklearn_transformers.py**.\n",
    "\n",
    "![alt text](https://i.imgur.com/2lZ4Ty2.png \"forking-a-repo\")\n",
    "\n",
    "Si solo hizo uso de la transformación proporcionada (DropColumns), puede omitir este paso de fork y continuar usando el paquete base provisto. :)\n",
    "\n",
    "Después de preparar su paquete de Python con sus transformaciones personalizadas, reemplace el enlace del repositorio de git en la celda a continuación y ejecútelo. Si no ha preparado ninguna transformación nueva, ejecute la celda con el enlace del repositorio ya proporcionado.\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**OBSERVACIÓN**\n",
    "\n",
    "Si la ejecución de la celda a continuación devuelve un error de que el repositorio ya existe, ejecute:\n",
    "\n",
    "**!rm -r -f sklearn_transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sklearn_transforms'...\n",
      "remote: Enumerating objects: 59, done.\u001b[K\n",
      "remote: Total 59 (delta 0), reused 0 (delta 0), pack-reused 59\u001b[K\n",
      "Unpacking objects: 100% (59/59), 9.48 KiB | 107.00 KiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# reemplace el enlace a continuación con el enlace de su repositorio de git (si corresponde)\n",
    "!git clone https://github.com/vnderlev/sklearn_transforms.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7652\n",
      "-rw-rw-r-- 1 ubuntu ubuntu    1860 Aug 16 00:58 NuevoNotebook.ipynb\n",
      "-rw-rw-r-- 1 ubuntu ubuntu    1890 Aug 16 01:02 Untitled.ipynb\n",
      "-rw-r--r-- 1 ubuntu ubuntu   52347 Aug 16 20:18 DataFile.data\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   26480 Aug 19 05:40 KSQLPrueba.ipynb\n",
      "-rw-r--r-- 1 ubuntu ubuntu   16830 Aug 19 17:04 Untitled1.ipynb\n",
      "-rw-r--r-- 1 ubuntu ubuntu    2439 Aug 20 02:02 TestBaseDBpsycopg2.ipynb\n",
      "-rw-r--r-- 1 ubuntu ubuntu     385 Aug 20 06:06 prueba_describe.csv\n",
      "drwxr-xr-x 2 ubuntu ubuntu    4096 Aug 20 06:07 outputs\n",
      "drwxr-xr-x 4 ubuntu ubuntu    4096 Aug 21 04:24 mlruns\n",
      "-rw-r--r-- 1 ubuntu ubuntu   22500 Aug 21 16:57 MLFlowEjemplo.ipynb\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2031803 Aug 29 01:34 dataset-tortuga-desafio-2.csv\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1751735 Sep  1 02:50 reto-4-compu-train.csv\n",
      "-rw-rw-r-- 1 ubuntu ubuntu   29334 Sep  2 11:50 des2-ori-parte-2.ipynb\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1751735 Sep  2 14:42 reto-4-compu-train.csv.1\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1751735 Sep  3 00:31 reto-4-compu-train.csv.2\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  239236 Sep  3 00:32 desafio2-ori-parte-1.ipynb\n",
      "drwxr-xr-x 3 ubuntu ubuntu    4096 Sep  3 00:32 respaldo\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  105330 Sep  3 00:33 des4-ori-compusoluciones.ipynb\n",
      "drwxr-xr-x 5 ubuntu ubuntu    4096 Sep  3 00:33 sklearn_transforms\n"
     ]
    }
   ],
   "source": [
    "!cd sklearn_transforms\n",
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar el código en WML, necesitamos enviar un archivo .zip con todo el código fuente, luego comprimiremos el directorio clonado a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: sklearn_transforms/ (stored 0%)\n",
      "  adding: sklearn_transforms/setup.cfg (deflated 16%)\n",
      "  adding: sklearn_transforms/setup.py (deflated 46%)\n",
      "  adding: sklearn_transforms/PKG-INFO (deflated 31%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/ (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/not-zip-safe (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/dependency_links.txt (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/PKG-INFO (deflated 33%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/SOURCES.txt (deflated 48%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms.egg-info/top_level.txt (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms/ (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms/__init__.py (stored 0%)\n",
      "  adding: sklearn_transforms/my_custom_sklearn_transforms/sklearn_transformers.py (deflated 46%)\n",
      "  adding: sklearn_transforms/README.md (deflated 15%)\n",
      "  adding: sklearn_transforms/.git/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/description (deflated 14%)\n",
      "  adding: sklearn_transforms/.git/branches/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/remotes/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/remotes/origin/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/remotes/origin/HEAD (deflated 30%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/heads/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/logs/refs/heads/master (deflated 30%)\n",
      "  adding: sklearn_transforms/.git/logs/HEAD (deflated 30%)\n",
      "  adding: sklearn_transforms/.git/refs/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/tags/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/remotes/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/remotes/origin/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/remotes/origin/HEAD (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/heads/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/refs/heads/master (stored 0%)\n",
      "  adding: sklearn_transforms/.git/hooks/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/hooks/commit-msg.sample (deflated 44%)\n",
      "  adding: sklearn_transforms/.git/hooks/post-update.sample (deflated 27%)\n",
      "  adding: sklearn_transforms/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
      "  adding: sklearn_transforms/.git/hooks/fsmonitor-watchman.sample (deflated 52%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-push.sample (deflated 50%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-rebase.sample (deflated 59%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
      "  adding: sklearn_transforms/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-commit.sample (deflated 45%)\n",
      "  adding: sklearn_transforms/.git/hooks/update.sample (deflated 68%)\n",
      "  adding: sklearn_transforms/.git/hooks/pre-receive.sample (deflated 40%)\n",
      "  adding: sklearn_transforms/.git/HEAD (stored 0%)\n",
      "  adding: sklearn_transforms/.git/config (deflated 34%)\n",
      "  adding: sklearn_transforms/.git/objects/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/54/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/54/a860e717a464beb16bcb21419ab86f8351fbf0 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/54/5d0fb908f733cd9eed69b0004a2dfeb33b5c48 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2c/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2c/c7a6bf2941a9d553a199a4fe658f0074a54b51 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2c/de64a17098cba673b28c995910b268f5e34e7a (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/e8/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/e8/8e9a7c9bc5a570d097ba4001ea0c327cba6f0a (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/82/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/82/e0b0b52c39cd1840e7853e6d00986fae5683f4 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/80/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/80/b20e55b5f52a87b7769e08df221e07d9cc0f3a (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/99/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/99/bde3a281e0a208866b47dcde00a4e4cd8afca0 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c4/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c4/b90bb9bfb2674c96e6d8aa6cc3c8e58194f953 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/da/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/da/64b79708ea475e8c6b484d837676ce91d9801e (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/58/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/58/25ba31fa0c00d63a9dc9865e0c848442a6fbcd (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/39/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/39/d4d3fd1cc23783dbadb2d673f3758863106854 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/39/ccfbaf81e116c9eb9781c7cd0b7acdd5ad10aa (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/77/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/77/7798a7cedea3a2fddef8dd0b7691c9b1f9f97b (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/8c/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/8c/46dcca21d6dd2c23afb4a858d2b77d56bbf050 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/8c/d09104c54f140d61a6e9fa6c2142d4565ee302 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/9b/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/9b/9fe6ce4d68c31dfc1eb7e923e5c90b0595b24d (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2f/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2f/ab3357d7d157a7a6bbda08aad6cf19970d2cc3 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c6/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c6/4e501fc43c00861aa7ccae744058ba87c304fd (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/5a/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/5a/b47e5cada9bcefbd66782d50edf2a86005dcb5 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/ee/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/ee/96aaec3459bae7b27b49e17f867db49c3257a8 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/f9/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/f9/842759dbd395649399f47479175e044b6b40ae (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/f2/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/f2/598626ccd27c586d5b759e5186abc64c058912 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/e6/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/af/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/af/1140cfcb4e33bc03d9b067c1f7d70147d043d7 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1b/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1b/9e7f9e69d0cac5283f551433cdaf8e65199698 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d7/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d7/cfed1a9cf21930beb933ecafb2e3d4d3a77874 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/61/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/61/e62b4f1623878ae0d92ffa812570be417feb82 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/aa/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/aa/1db9544c86800c5018cb8f28d1e74fbd192991 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/21/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/21/64a6e4b4ca3afc26c4b21982d433c8c54039a1 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/04/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/04/b1ff2d79c9b11a1d494b6f6d42acb629236af4 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/7b/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/7b/04014fbd820cb0146dcec7965f01f304436510 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/a0/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/a0/3066ce73d15efd1cc6449dea7e2b678089edd6 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/8b/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/8b/137891791fe96927ad78e64b0aad7bded08bdc (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c3/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c3/9f70251de841c8e55e9c58d2fa7dbbe2d2fb0f (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1c/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1c/49d4656922fe1182c905339ec7a468e6881d99 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/pack/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/85/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/85/bd34a4ae5f5c965a082a26b38a3268a662ae2b (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/info/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/22/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/22/2e658014f5f2a43e16ce3c5e896e00fef1f3d5 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/cc/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/cc/fdab500390127d0490aaf9ee369565b05c41b4 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1e/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/1e/1e45038dfc30ed50e6d081ec85e8a3fdfb8913 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d3/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d3/697484c118054fc68a13eaac93c2f93536b77e (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/6a/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/6a/3c980900b1ce8383ed0fd1cbcb9195d582cf4e (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/24/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/24/7b371c9092731e07412dc5286126ed3a18756a (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c0/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/c0/e12da6a090fe6b2740d0cbfa7119915a00a643 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/bc/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/bc/a817006f92d9e390374c1ed6467fad5ff7bab0 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/bc/bd8f17177cd1afe8396315d2039283fa993390 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d0/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/d0/4d8adf3dbf886c5799f7f367168b0785f0a4f5 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/26/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/26/3abe321ee224139817e2325d5c91ac8f0eed50 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/06/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/06/998ae152d5b5fad89c81a6853dbbc0830fd6c7 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/cb/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/cb/d827fdd178a26f42d8b01c58320b03f3eae7fa (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/29/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/29/c65b6319ccd5b1c7da36655bb0c44e09fff991 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/29/ffc1c2af8fa4b0ef5eef0ba4fdc441915d1a91 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/05/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/05/6cab60f81eb62f20ffd3cab59b6d232f0188e9 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/46/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/46/862abd9b50e51c99a1f65be820af360594f8c2 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/b6/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/b6/979f3149443c66198e505c1e761f2766b4ea35 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/52/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/52/6fb1c4f5a0d3a2482041c7bd3f15263837acf5 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/0e/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/0e/cc61a0a4baff0885426f45a8fcb9fbb78f8009 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2e/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/2e/2e5168d3c37ccbaeb3b899da3f30fb6bc70031 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/7e/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/objects/7e/ae3d08dc940e367e3c1e2ac7a35a8b00df2e12 (stored 0%)\n",
      "  adding: sklearn_transforms/.git/packed-refs (deflated 11%)\n",
      "  adding: sklearn_transforms/.git/info/ (stored 0%)\n",
      "  adding: sklearn_transforms/.git/info/exclude (deflated 28%)\n",
      "  adding: sklearn_transforms/.git/index (deflated 52%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r sklearn_transforms.zip sklearn_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el archivo zip de nuestro paquete cargado en el Kernel de este notebook, podemos usar la herramienta pip para instalarlo, de acuerdo con la celda a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./sklearn_transforms.zip\n",
      "Building wheels for collected packages: my-custom-sklearn-transforms\n",
      "  Building wheel for my-custom-sklearn-transforms (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for my-custom-sklearn-transforms: filename=my_custom_sklearn_transforms-1.0-py3-none-any.whl size=2049 sha256=4b16d55468be0e7ff4bdd195aa76c8c80d628df4440615be224ba915be85d27f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m_ui458z/wheels/ae/29/e1/c3010d0922fafae3ca9680ba834f385903be2d10aa80007fa3\n",
      "Successfully built my-custom-sklearn-transforms\n",
      "Installing collected packages: my-custom-sklearn-transforms\n",
      "Successfully installed my-custom-sklearn-transforms-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn_transforms.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ahora podemos importar nuestro paquete personalizado a nuestro notebook!\n",
    "\n",
    "Importaremos la transformación DropColumns. Si tienes otras transformaciones personalizadas, ¡no olvides importarlas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_custom_sklearn_transforms.sklearn_transformers import DropColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declarando un Pipeline\n",
    "\n",
    "Después de importar transformaciones personalizadas como un paquete de Python, podemos proceder a la declaración de nuestro Pipeline.\n",
    "\n",
    "El proceso es muy similar al realizado en la primera etapa, pero con algunas diferencias importantes, ¡así que presta mucha atención!\n",
    "\n",
    "El Pipeline de ejemplo tiene tres etapas: \n",
    "\n",
    "    - Remover las colunas \"NAME\" e \"Unnamed: 0\"\n",
    "    - Asignar \"ceros\" a todos los valores faltantes\n",
    "    - insertar datos preprocesados como entrada en un modelo entrenado\n",
    "    \n",
    "Recordando, la entrada de este Pipeline será el conjunto de datos brutos proporcionados, excepto la columna \"PROFILE\" (variable de objetivo que será determinada por el modelo).\n",
    "\n",
    "Entonces tendremos 16 valores de entrada en el **PIPELINE** (en el modelo habrá 14 entradas, ya que las columnas \"NAME\" y \"Unnamed: 0\" se eliminarán en la primera etapa después de la transformación DropColumns).\n",
    "\n",
    "\n",
    "    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n",
    "    NAME                                - Nombre del estudiante\n",
    "    USER_ID                             - Número de identificación del estudiante\n",
    "    HOURS_DATASCIENCE                   - Número de horas de estudio en Data Science\n",
    "    HOURS_BACKEND                       - Número de horas de estudio en Web (Back-End)\n",
    "    HOURS_FRONTEND                      - Número de horas de estudio en Web (Front-End)\n",
    "    NUM_COURSES_BEGINNER_DATASCIENCE    - Número de cursos de nivel principiante en Data Science completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_BACKEND        - Número de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_BEGINNER_FRONTEND       - Número de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_DATASCIENCE    - Número de cursos de nivel avanzado en Data Science completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_BACKEND        - Número de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n",
    "    NUM_COURSES_ADVANCED_FRONTEND       - Número de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n",
    "    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n",
    "    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n",
    "    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n",
    "\n",
    "La salida del Pipeline será un valor estimado para la columna \"PROFILE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes del entorno\n",
    "Se crea una semilla aleatoria para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilla_aleatoria = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una transformación personalizada ``DropColumns``\n",
    "\n",
    "rm_columns = DropColumns(\n",
    "    columns=[\"NAME\", \"Unnamed: 0\",\"USER_ID\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto ``SimpleImputer``\n",
    "num_features = ['HOURS_DATASCIENCE', 'HOURS_BACKEND', 'HOURS_FRONTEND',\n",
    "       'NUM_COURSES_BEGINNER_DATASCIENCE', 'NUM_COURSES_BEGINNER_BACKEND',\n",
    "       'NUM_COURSES_BEGINNER_FRONTEND', 'NUM_COURSES_ADVANCED_DATASCIENCE',\n",
    "       'NUM_COURSES_ADVANCED_BACKEND', 'NUM_COURSES_ADVANCED_FRONTEND',\n",
    "       'AVG_SCORE_DATASCIENCE', 'AVG_SCORE_BACKEND', 'AVG_SCORE_FRONTEND']\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de columnas que seran features\n",
    "features = [\n",
    "    \"Unnamed: 0\", \"NAME\", \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n",
    "    \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n",
    "    \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n",
    "    \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n",
    "]\n",
    "\n",
    "# Definición de variable objetico\n",
    "target = [\"PROFILE\"]\n",
    "\n",
    "# Preparación de argumentos para los métodos de la biblioteca ``scikit-learn``\n",
    "X = df_data_1[features]\n",
    "y = df_data_1[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡¡ATENCIÓN!!**\n",
    "\n",
    "La celda de arriba, aunque muy similar a la definición de características en la primera etapa de este desafío, ¡tiene una gran diferencia!\n",
    "\n",
    "Contiene las columnas \"NAME\" y \"Unnamed: 0\" como features. Esto se debe a que en este caso estas son las entradas *PIPELINE*, no el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en un conjunto de entrenamiento y un conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda a continuación, se declara un objeto scikit-learn **Pipeline**, donde se declara el parámetro *steps*, que no es más que una lista de los pasos en nuestro pipeline:\n",
    "\n",
    "    'remove_cols'     - transformación personalizada DropColumns\n",
    "    'imputer'         - transformación scikit-learn incorporada para asignar valores faltantes\n",
    "    'dtc'             - un clasificador a través del árbol de decisión\n",
    "\n",
    "Tenga en cuenta que pasamos como transformaciones instanciadas anteriormente, bajo el nombre `rm_columns` y` si`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de nuestro pipeline para almacenamiento en Watson Machine Learning:\n",
    "\n",
    "class_SVC = SVC(C=1000,kernel='rbf',random_state=semilla_aleatoria,gamma='auto')\n",
    "class_RFO = RandomForestClassifier(n_estimators=800, min_samples_split=2,min_samples_leaf= 1,max_features= 'auto',max_depth=30, bootstrap=True, random_state=semilla_aleatoria)\n",
    "class_RNN = MLPClassifier(solver= 'sgd', max_iter= 480, learning_rate= 'adaptive', hidden_layer_sizes= (144, 144, 12), alpha= 0.05, activation='tanh', random_state=semilla_aleatoria)\n",
    "voting_clf = VotingClassifier(estimators=[('SVC', class_SVC), ('RFOR', class_RFO),('RNN',class_RNN)], voting='hard')\n",
    "clf = Pipeline(steps=[('removecols',rm_columns),\n",
    "                      ('preprocessor', preprocessor),\n",
    "                      ('clf', voting_clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, ejecutaremos el método `fit ()` de Pipeline, realizando el preprocesamiento y entrenamiento del modelo a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (480) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('removecols',\n",
       "                 DropColumns(columns=['NAME', 'Unnamed: 0', 'USER_ID'])),\n",
       "                ('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['HOURS_DATASCIENCE',\n",
       "                                                   'HOURS_BACKEND',\n",
       "                                                   'HOURS_FRONTEND',\n",
       "                                                   'NUM_COURSES_BEGINNER_DATASCIENCE',\n",
       "                                                   'NUM_COURSES_BEGINNER_BACKEND',\n",
       "                                                   '...\n",
       "                                                   'AVG_SCORE_FRONTEND'])])),\n",
       "                ('clf',\n",
       "                 VotingClassifier(estimators=[('SVC',\n",
       "                                               SVC(C=1000, gamma='auto',\n",
       "                                                   random_state=1234)),\n",
       "                                              ('RFOR',\n",
       "                                               RandomForestClassifier(max_depth=30,\n",
       "                                                                      n_estimators=800,\n",
       "                                                                      random_state=1234)),\n",
       "                                              ('RNN',\n",
       "                                               MLPClassifier(activation='tanh',\n",
       "                                                             alpha=0.05,\n",
       "                                                             hidden_layer_sizes=(144,\n",
       "                                                                                 144,\n",
       "                                                                                 12),\n",
       "                                                             learning_rate='adaptive',\n",
       "                                                             max_iter=480,\n",
       "                                                             random_state=1234,\n",
       "                                                             solver='sgd'))]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicialización de Pipeline (preprocesamiento y formación de modelos)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un pipeline completo, con los pasos de preprocesamiento configurados y también un modelo por árbol de decisiones ya entrenado, ¡podemos integrarnos con Watson Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encapsulación de un Pipeline personalizado en Watson Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establecer una conexión entre el cliente WML Python y su instancia de servicio en la nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca de Python con implementación de un cliente HTTP para la API de WML\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes celdas desplegarán el pipeline declarado en este notebook en WML. Continúe solo si ya está satisfecho con su modelo y cree que es hora de implementar su solución.\n",
    "\n",
    "Pegue las credenciales de su instancia de Watson Machine Learning en la variable de la celda siguiente.\n",
    "\n",
    "Es importante que la variable que contiene los valores tenga el nombre de `` wml_credentials`` para que las siguientes celdas de este notebook se ejecuten correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "  \"apikey\": \"\",\n",
    "  \"iam_apikey_description\": \"\",\n",
    "  \"iam_apikey_name\": \"\",\n",
    "  \"iam_role_crn\": \"\",\n",
    "  \"iam_serviceid_crn\": \"\",\n",
    "  \"instance_id\": \"\",\n",
    "  \"url\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un objeto cliente de Watson Machine Learning a partir de las credenciales proporcionadas\n",
    "\n",
    "clientWML = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATENCIÓN, SI EJECUTA LAS 2 CELDAS SIGUIENTES BORRARÁ LOS DEPLIEGUES ANTERIORES DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda para borrar los Deployments:\n",
    "\n",
    "for uid in clientWML.deployments.get_uids():\n",
    "    if clientWML.deployments.get_details(uid)['entity']['name'] == 'desafio-2-mbtc2020-deployment-es-2' :\n",
    "        print('Deleted ' + clientWML.deployments.get_details(uid)['entity']['name'] )\n",
    "        clientWML.deployments.delete(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Celda para borrar todos los recursos:\n",
    "\n",
    "d = clientWML.repository.get_details()\n",
    "\n",
    "for k in d:\n",
    "    for res in d[k][\"resources\"]:\n",
    "        if res['entity']['name'] in ['my_custom_wml_runtime_es_2', 'my_custom_sklearn_transform_es_2', 'desafio-2-mbtc2020-pipeline-es-2']:\n",
    "            clientWML.repository.delete(res[\"metadata\"][\"guid\"])\n",
    "            print('Deleted ' + res['entity']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracción de detalles de su instancia de Watson Machine Learning\n",
    "\n",
    "instance_details = clientWML.service_instance.get_details()\n",
    "print(json.dumps(instance_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡¡ATENCIÓN!!**\n",
    "\n",
    "¡Preste atención a los límites de consumo de su instancia de Watson Machine Learning!\n",
    "\n",
    "Si expira la capa libre, no podrá evaluar su modelo (¡ya que es necesario realizar algunas llamadas a API que consumen predicciones!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listado de todos los artefactos almacenados en su WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para listar todos los artefactos almacenados en Watson Machine Learning, puede utilizar la siguiente función:\n",
    "\n",
    "    clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de todos los artefactos almacenados en su WML\n",
    "\n",
    "clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el plan LITE de Watson Machine Learning, solo se puede implementar un modelo a la vez. Si ya tiene un modelo en línea en su instancia, puede eliminarlo usando el método clientWML.repository.delete ():\n",
    "\n",
    "    artifact_guid = \"359c8951-d2fe-4063-8706-cc06b32d5e0d\"\n",
    "    clientWML.repository.delete(artifact_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear una nueva definición de paquete Python personalizada en WML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso para realizar su implementación es almacenar el código de las transformaciones personalizadas creadas.\n",
    "\n",
    "Para este paso solo necesitamos el archivo .zip del paquete creado (¡que ya hemos cargado en el Kernel!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de metadatos de nuestro paquete con transformaciones personalizadas \n",
    "pkg_meta = {\n",
    "    clientWML.runtimes.LibraryMetaNames.NAME: \"my_custom_sklearn_transform_es_2\",\n",
    "    clientWML.runtimes.LibraryMetaNames.DESCRIPTION: \"A custom sklearn transform\",\n",
    "    clientWML.runtimes.LibraryMetaNames.FILEPATH: \"sklearn_transforms.zip\",  # Note que estamos utilizando o .zip creado anteriormente!\n",
    "    clientWML.runtimes.LibraryMetaNames.VERSION: \"1.0\",\n",
    "    clientWML.runtimes.LibraryMetaNames.PLATFORM: { \"name\": \"python\", \"versions\": [\"3.6\"] }\n",
    "}\n",
    "custom_package_details = clientWML.runtimes.store_library( pkg_meta )\n",
    "custom_package_uid = clientWML.runtimes.get_library_uid( custom_package_details )\n",
    "\n",
    "print(\"\\n Lista de artefactos en tiempo de ejecución almacenados en WML:\")\n",
    "clientWML.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de una nueva definición personalizada de runtime Python en WML\n",
    "\n",
    "El segundo paso es almacenar una definición de runtime Python para usar nuestra biblioteca personalizada.\n",
    "\n",
    "Esto puede hacerse de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_meta = {\n",
    "    clientWML.runtimes.ConfigurationMetaNames.NAME: \"my_custom_wml_runtime_es_2\",\n",
    "    clientWML.runtimes.ConfigurationMetaNames.DESCRIPTION: \"A Python runtime with custom sklearn Transforms\",\n",
    "    clientWML.runtimes.ConfigurationMetaNames.PLATFORM: {\n",
    "        \"name\": \"python\",\n",
    "        \"version\": \"3.6\"\n",
    "    },\n",
    "    clientWML.runtimes.ConfigurationMetaNames.LIBRARIES_UIDS: [ custom_package_uid ]\n",
    "}\n",
    "runtime_details = clientWML.runtimes.store( runtime_meta )\n",
    "custom_runtime_uid = clientWML.runtimes.get_uid( runtime_details )\n",
    "\n",
    "print(\"\\n Detalles de runtime almacenados:\")\n",
    "print(json.dumps(runtime_details, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando todos runtimes armazenados no seu WML:\n",
    "clientWML.runtimes.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear una nueva definición de Pipeline personalizado en WML\n",
    "\n",
    "Finalmente crearemos una definición (metadatos) para que nuestro Pipeline se aloje en WML.\n",
    "\n",
    "Definimos como parámetros un nombre para el artefacto y el ID de runtime creado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    clientWML.repository.ModelMetaNames.NAME: 'desafio-2-mbtc2020-pipeline-es-2',\n",
    "    clientWML.repository.ModelMetaNames.DESCRIPTION: \"my pipeline for submission\",\n",
    "    clientWML.repository.ModelMetaNames.RUNTIME_UID: custom_runtime_uid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego llamamos al método para almacenar la nueva definición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para almacenar una definición de Pipeline en WML \n",
    "stored_model_details = clientWML.repository.store_model(\n",
    "    model=my_pipeline,  # `my_pipeline` es la variable creada previamente y contiene nuestro Pipeline ya entrenado :)\n",
    "    meta_props=model_meta,  # Metadatos definidos en la celda anterior\n",
    "    training_data=None  # No cambie este parámetro\n",
    ")\n",
    "\n",
    "print(\"\\n Lista de artefactos almacenados en WML:\")\n",
    "clientWML.repository.list()\n",
    "\n",
    "# Detalles del modelo alojado en Watson Machine Learning\n",
    "print(\"\\n Metadatos almacenados del modelo:\")\n",
    "print(json.dumps(stored_model_details, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despliegue de su modelo para consumo inmediato por otras aplicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo finalmente se implementa usando el método `` deployments.create () ``\n",
    "\n",
    "model_deployment_details = clientWML.deployments.create(\n",
    "    artifact_uid=stored_model_details[\"metadata\"][\"guid\"],  # No cambie este parámetro\n",
    "    name=\"desafio-2-mbtc2020-deployment-es-2\",\n",
    "    description=\"Solução do desafio 2 - MBTC\",\n",
    "    asynchronous=False,  # No cambie este parámetro\n",
    "    deployment_type='online',  # No cambie este parámetro\n",
    "    deployment_format='Core ML',  # No cambie este parámetro\n",
    "    meta_props=model_meta  # No cambie este parámetro\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba de un modelo alojado en Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando a URL endpoint do modelo hospedado na célula anterior\n",
    "\n",
    "model_endpoint_url = clientWML.deployments.get_scoring_url(model_deployment_details)\n",
    "print(\"Su URL de llamada a la API es: {}\".format(model_endpoint_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalles del despliegue realizado\n",
    "\n",
    "deployment_details = clientWML.deployments.get_details(\n",
    "    deployment_uid=model_deployment_details[\"metadata\"][\"guid\"]  # este es su ID de implementación!\n",
    ")\n",
    "\n",
    "print(\"Metadatos de despliegue realizado: \\n\")\n",
    "print(json.dumps(deployment_details, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_payload = {\n",
    "    'fields': [\n",
    "        \"Unnamed: 0\", \"NAME\", \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n",
    "        \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n",
    "        \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n",
    "        \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n",
    "    ],\n",
    "    'values': [\n",
    "        [\n",
    "            0,\"Paula Waters\",85123728,0.0,0.0,86.0,72.0,42.0,0.0,0.0,26.0,184.0,37.0,63.0,38.0,\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Definición de la variable objetivo\n",
    "target = [\"PROFILE\"]\n",
    "\n",
    "print(\"\\n Payload de datos para clasificar:\")\n",
    "print(json.dumps(scoring_payload, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clientWML.deployments.score(\n",
    "    model_endpoint_url,\n",
    "    scoring_payload\n",
    ")\n",
    "\n",
    "print(\"\\n Resultados:\")\n",
    "print(json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## ¡Felicidades! \n",
    "\n",
    "Si todo salió bien, ¡ya tiene un clasificador basado en aprendizaje automático encapsulado como una API REST!\n",
    "\n",
    "Para probar tu solución integrada con un asistente virtual y realizar el envío, visita la página:\n",
    "\n",
    "https://tortuga.maratona.dev\n",
    "\n",
    "Necesitarás la URL del endpoint de tu modelo y las credenciales WML :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
